{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './surnames/'\n",
    "Chinese = open(root+'Chinese.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "Japanese = open(root+'Japanese.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "Korean = open(root+'Korean.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "English = open(root+'English.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "Irish = open(root+'Irish.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "Russian = open(root+'Russian.txt', encoding='utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([2, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "#1 - a\n",
    "import string\n",
    "## We'll consider all ascii letters plus basic punctuation\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "all_letters = {character : index for index, character in enumerate(all_letters)}\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "## Function to iterate through a line of text encode each letter as a 1 x 57 vector in an nchar x 1 x 57 tensor\n",
    "def nameToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        if letter in all_letters:\n",
    "            tensor[li][0][all_letters[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "## Demonstration of the test name \"Aa\", notice the \"A\" is encoded as the 27th position, and \"a\" is the 1st position\n",
    "example = nameToTensor('Aa')\n",
    "print(example)\n",
    "\n",
    "## Also notice dim1 of the tensor is the number of charactersr in the name\n",
    "print(example.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 - a\n",
    "the first dimension represents the lengh of an input in terms of the character\n",
    "if we give different input, then depending on the input, the first dimension might change. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 - b\n",
    "the third dimension represents the indices of the characters in the input. \n",
    "if we give different input, the third dimension will not change. \n",
    "\n",
    "#2 - a\n",
    "\n",
    "6 comes from the number of languages (number of classes) for our data. \n",
    "we can technically change it, but it will harm our accuracy. \n",
    "\n",
    "#2 - b\n",
    "\n",
    "100 comes from the dimension of $a^{<0>}$. This should a parameter we can change in the architecture. \n",
    "\n",
    "#3 - a\n",
    "the input of the loop is the character array of all_letters\n",
    "\n",
    "#3 - b\n",
    "the output would be the probability that the ietters upto the loop's  iteration belonging to the languages. \n",
    "\n",
    "#3 - c\n",
    "We reinitialize hidden state because every time we are training a new model, we want a zeroed hidden states. Not the hidden state from the previous model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9449, -1.6718, -1.8236, -1.7353, -1.7957, -1.8001]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class my_rnn(nn.Module):\n",
    "    \n",
    "    ## Constructor commands\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(my_rnn, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    ## Function to generate predictions\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "## Initialize model with random weights\n",
    "rnn = my_rnn(n_letters, 100, 6)\n",
    "\n",
    "## Format an example input name (Albert)\n",
    "test_input = nameToTensor('Albert')\n",
    "\n",
    "## Provide an initial hidden state (all zeros this time)\n",
    "hidden = torch.zeros(1, 100)\n",
    "\n",
    "## Generate output from the RNN\n",
    "output, next_hidden = rnn(test_input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of categories\n",
    "category_labels = ['Chinese', 'Japanese', 'Korean', 'English', 'Irish', 'Russian']\n",
    "\n",
    "## Dictionary of categories and names\n",
    "category_lines = {'Chinese': Chinese,\n",
    "                 'Japanese': Japanese,\n",
    "                 'Korean': Korean,\n",
    "                 'English': English,\n",
    "                 'Irish': Irish,\n",
    "                 'Russian': Russian}\n",
    "\n",
    "# Function to randomly sample a single example\n",
    "import random\n",
    "def randomTrainingExample():\n",
    "    ## Randomly choose a category index (ie: Chinese, etc.)\n",
    "    category = category_labels[random.randint(0, len(category_labels)-1)]\n",
    "    \n",
    "    ## Randomly choose a name in that category\n",
    "    name = category_lines[category][random.randint(0, len(category_lines[category])-1)]\n",
    "    \n",
    "    ## Convert the chosen example to a tensor\n",
    "    category_tensor = torch.tensor([category_labels.index(category)], dtype=torch.long)\n",
    "    line_tensor = nameToTensor(name)\n",
    "    \n",
    "    return category, name, category_tensor, line_tensor\n",
    "\n",
    "\n",
    "## Set learning rate\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Define cost func\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "## Training function for a single input (name category, name)\n",
    "def train(category_tensor, line_tensor):\n",
    "    \n",
    "    ## initialize the hidden state\n",
    "    hidden = rnn.initHidden()\n",
    "    \n",
    "    ## set the gradient to zero\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    ## loop through the letters in the input, getting a prediction and new hidden state each time\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    ## Calculate cost and gradients\n",
    "    cost = cost_fn(output, category_tensor)\n",
    "    cost.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha = -learning_rate) ## This adds the LR times the gradient to each parameter \n",
    "\n",
    "    ## Return the output and cost\n",
    "    return output, cost.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializations\n",
    "n_iters = 10000\n",
    "cost_every_n = 25\n",
    "current_cost = 0\n",
    "track_cost = []\n",
    "\n",
    "### Iteratively update model from randomly chosen example\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, cost = train(category_tensor, line_tensor)\n",
    "    current_cost += cost\n",
    "    \n",
    "    # Save cost every 25 iterations\n",
    "    if iter % cost_every_n == 0:\n",
    "        track_cost.append(current_cost/cost_every_n)\n",
    "        current_cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Gorbachev\n",
      "(-0.05) Russian\n",
      "(-3.83) Irish\n",
      "(-3.84) English\n",
      "(-5.46) Japanese\n",
      "\n",
      "> Chris\n",
      "(-0.98) Russian\n",
      "(-1.37) English\n",
      "(-1.70) Korean\n",
      "(-2.19) Irish\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "\n",
    "def predict(input_line, n_predictions=4):\n",
    "    print('\\n> %s' % input_line)\n",
    "    \n",
    "    ## Don't update gradient with any of these examples\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ## Initialize new hidden state\n",
    "        hidden = rnn.initHidden()\n",
    "        \n",
    "        ## Convert input str to tensor\n",
    "        input_t = nameToTensor(input_line)\n",
    " \n",
    "        ## Pass each character into `rnn`\n",
    "        for i in range(input_t.size()[0]):\n",
    "            output, hidden = rnn(input_t[i], hidden)\n",
    "\n",
    "        # Get top N categories from output\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        ## Go through the category predictions and save info for printing\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, category_labels[category_index]))\n",
    "            predictions.append([value, category_labels[category_index]])\n",
    "\n",
    "## Try it out on a few examples:\n",
    "predict('Gorbachev')\n",
    "predict('Chris')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we expected this result to happen. The reason is because in general, the model predicts pretty accurately. However, when the margin is small, the prediction might be wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "n_categories = len(category_labels)\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1\n",
    "\n",
    "class my_gen_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(my_gen_rnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = torch.cat((category, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def outputTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) \n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "def categoryTensor(category):\n",
    "    li = category_labels.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(category_labels)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line\n",
    "\n",
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = outputTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15\n",
    "gen_rnn = my_gen_rnn(n_letters, 128, n_letters)\n",
    "\n",
    "# Sample using a given category and starting letter\n",
    "def sample(category, start_letter):\n",
    "    \n",
    "    ## We are just sampling, so we don't want to store info used in gradient calculations\n",
    "    with torch.no_grad(): \n",
    "        category_tensor = categoryTensor(category)  ## create category tensor of input category\n",
    "        input = inputTensor(start_letter)           ## intialize input tensor as an encoding of the start letter\n",
    "        hidden = gen_rnn.initHidden()               ## reset the initial hidden state\n",
    "        output_name = start_letter                  ## Use start letter as first piece of the output name\n",
    "        \n",
    "        ## Loop until reaching the max length or the stop character \n",
    "        for i in range(max_length):\n",
    "            output, hidden = gen_rnn(category_tensor, input[0], hidden)  ## Get the next output and hidden state\n",
    "            topv, topi = output.topk(1)                                  ## Identify the top predicted character's value and index position\n",
    "            topi = topi[0][0]                                            ## Extract integer id of predicted char\n",
    "            if topi == n_letters - 1:                                    ## Stop if its the stop character's ID\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]                               ## Convert integer id to the character\n",
    "                output_name += letter                                    ## Add this character to the output \n",
    "            input = inputTensor(letter)                                  ## Prep this letter as the next input\n",
    "\n",
    "        return output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hni ttOOOOOOOOOO'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample('Korean', 'H')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-A\n",
    "we would expect the same result in other languages. \n",
    "the reason is because we did not trained our model. \n",
    "# 5-B\n",
    "\n",
    "Because even if we have not reach the stop word, we want to break when we have a stop character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_fn = nn.CrossEntropyLoss()\n",
    "gen_rnn = my_gen_rnn(n_letters, 128, n_letters)\n",
    "learning_rate = 0.001\n",
    "\n",
    "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = gen_rnn.initHidden()\n",
    "\n",
    "    gen_rnn.zero_grad()\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = gen_rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "        l = cost_fn(output, target_line_tensor[i])\n",
    "        cost += l\n",
    "\n",
    "    cost.backward()\n",
    "\n",
    "    for p in gen_rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, cost.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "cost_every_n = 25\n",
    "current_cost = 0\n",
    "track_cost = []\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    cat, il, ol = randomTrainingExample()\n",
    "    if -1 in ol:                               ### If an example happens to contain an unusual character we'll skip it\n",
    "        continue  \n",
    "    output, cost = train(cat, il, ol)\n",
    "    current_cost += cost\n",
    "    \n",
    "    # Save the cost every 25 iterations\n",
    "    if iter % cost_every_n == 0:\n",
    "        track_cost.append(current_cost/cost_every_n)\n",
    "        current_cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean: Ron \n",
      "Japanese: Rano \n",
      "Chinese: Ran \n",
      "English: Rarana \n",
      "Irish: Ranen \n",
      "Russian: Raronon\n"
     ]
    }
   ],
   "source": [
    "test_letter = 'R'\n",
    "print('Korean:',sample('Korean', test_letter), \n",
    "      '\\nJapanese:', sample('Japanese', test_letter),\n",
    "      '\\nChinese:', sample('Chinese', test_letter),\n",
    "      '\\nEnglish:', sample('English', test_letter),\n",
    "      '\\nIrish:', sample('Irish', test_letter),\n",
    "      '\\nRussian:', sample('Russian', test_letter))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we believe that the code was pretty effective (in terms of the chinese and korean point of view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
